{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing Generator...\n",
      "Initializing Generator...\n"
     ]
    }
   ],
   "source": [
    "#Import libs\n",
    "import time\n",
    "import os, os.path\n",
    "import random\n",
    "import cv2\n",
    "import matplotlib\n",
    "import functools\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from Data_Generator import data_generator\n",
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "#Hyperparameters\n",
    "\n",
    "batch_size = 50\n",
    "Alpha = 0.5\n",
    "margin = Alpha\n",
    "\n",
    "# Generators\n",
    "training_generator = data_generator('./test_example.txt',batch_size,True).generate()\n",
    "test_generator = data_generator('./test_example.txt',batch_size,True).generate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#No need for positive mask in Multimodal Triplet. Anchor and Positives will be fed through data inputs.'''\n",
    "def _pairwise_distances(image_embeddings, text_embeddings, squared=False):\n",
    "    \"\"\"Compute the 2D matrix of distances between all the embeddings.\n",
    "    Args:\n",
    "        embeddings: tensor of shape (batch_size, embed_dim)\n",
    "        squared: Boolean. If true, output is the pairwise squared euclidean distance matrix.\n",
    "                 If false, output is the pairwise euclidean distance matrix.\n",
    "    Returns:\n",
    "        pairwise_distances: tensor of shape (batch_size, batch_size)\n",
    "    \"\"\"\n",
    "    # Get the dot product between all embeddings\n",
    "    # shape (batch_size, batch_size)\n",
    "    dot_product11 = tf.matmul(image_embeddings, tf.transpose(image_embeddings))\n",
    "    dot_product22 = tf.matmul(text_embeddings, tf.transpose(text_embeddings))\n",
    "    dot_product12 = tf.matmul(image_embeddings, tf.transpose(text_embeddings))\n",
    "\n",
    "    # Get squared L2 norm for each embedding. We can just take the diagonal of `dot_product`.\n",
    "    # This also provides more numerical stability (the diagonal of the result will be exactly 0).\n",
    "    # shape (batch_size,)\n",
    "    square_norm11 = tf.diag_part(dot_product11)\n",
    "    square_norm22 = tf.diag_part(dot_product22)\n",
    "\n",
    "    # Compute the pairwise distance matrix as we have:\n",
    "    # ||a - b||^2 = ||a||^2  - 2 <a, b> + ||b||^2\n",
    "    # shape (batch_size, batch_size)\n",
    "    distances = tf.expand_dims(square_norm11, 0) - 2.0 * dot_product12 + tf.expand_dims(square_norm22, 1)\n",
    "\n",
    "    # Because of computation errors, some distances might be negative so we put everything >= 0.0\n",
    "    distances = tf.maximum(distances, 0.0)\n",
    "\n",
    "    if not squared:\n",
    "        # Because the gradient of sqrt is infinite when distances == 0.0 (ex: on the diagonal)\n",
    "        # we need to add a small epsilon where distances == 0.0\n",
    "        mask = tf.to_float(tf.equal(distances, 0.0))\n",
    "        distances = distances + mask * 1e-16\n",
    "\n",
    "        distances = tf.sqrt(distances)\n",
    "\n",
    "        # Correct the epsilon added: set the distances on the mask to be exactly 0.0\n",
    "        distances = distances * (1.0 - mask)\n",
    "\n",
    "    return distances\n",
    "#No need for positive mask in Multimodal Triplet. Anchor and Positives will be fed through data inputs.\n",
    "\n",
    "def _get_anchor_negative_triplet_mask(labels):\n",
    "    \"\"\"Return a 2D mask where mask[a, n] is True iff a and n have distinct labels.\n",
    "    Args:\n",
    "        labels: tf.int32 `Tensor` with shape [batch_size]\n",
    "    Returns:\n",
    "        mask: tf.bool `Tensor` with shape [batch_size, batch_size]\n",
    "    \"\"\"\n",
    "    # Check if labels[i] != labels[k]\n",
    "    # Uses broadcasting where the 1st argument has shape (1, batch_size) and the 2nd (batch_size, 1)\n",
    "    labels_equal = tf.equal(tf.expand_dims(labels, 0), tf.expand_dims(labels, 1))\n",
    "\n",
    "    mask = tf.logical_not(labels_equal)\n",
    "\n",
    "    return mask\n",
    "\n",
    "\n",
    "def _get_triplet_mask(labels):\n",
    "    \"\"\"Return a 3D mask where mask[a, p, n] is True iff the triplet (a, p, n) is valid.\n",
    "    A triplet (i, j, k) is valid if:\n",
    "        - i == j, k is distinct\n",
    "        - i == j and labels[i] != labels[k]\n",
    "    Args:\n",
    "        labels: tf.int32 `Tensor` with shape [batch_size]\n",
    "    \"\"\"\n",
    "    # Check that i == j and k is distinct\n",
    "    indices_equal = tf.cast(tf.eye(tf.shape(labels)[0]), tf.bool)\n",
    "    indices_not_equal = tf.logical_not(indices_equal)\n",
    "    i_equal_j = tf.expand_dims(indices_equal, 2)\n",
    "    i_not_equal_k = tf.expand_dims(indices_not_equal, 1)\n",
    "    j_not_equal_k = tf.expand_dims(indices_not_equal, 0)\n",
    "\n",
    "    valid_indices = tf.logical_and(tf.logical_and(i_equal_j, i_not_equal_k), j_not_equal_k)\n",
    "\n",
    "\n",
    "    # Check if labels[i] == labels[j] and labels[i] != labels[k]\n",
    "    label_equal = tf.equal(tf.expand_dims(labels, 0), tf.expand_dims(labels, 1))\n",
    "    i_equal_j = tf.expand_dims(label_equal, 2)\n",
    "    i_equal_k = tf.expand_dims(label_equal, 1)\n",
    "\n",
    "    valid_labels = tf.logical_and(i_equal_j, tf.logical_not(i_equal_k))\n",
    "\n",
    "    # Combine the two masks\n",
    "    mask = tf.logical_and(valid_indices, valid_labels)\n",
    "\n",
    "    return mask\n",
    "\n",
    "#No need for triplet mask. Negative Mask, serves as triplet mask for both L_tii and L_itt.\n",
    "\n",
    "def batch_all_triplet_loss(labels, image_embeddings, text_embeddings, margin, squared=False):\n",
    "    \"\"\"Build the triplet loss over a batch of embeddings.\n",
    "    We generate all the valid triplets and average the loss over the positive ones.\n",
    "    Args:\n",
    "        labels: labels of the batch, of size (batch_size,)\n",
    "        embeddings: tensor of shape (batch_size, embed_dim)\n",
    "        margin: margin for triplet loss\n",
    "        squared: Boolean. If true, output is the pairwise squared euclidean distance matrix.\n",
    "                 If false, output is the pairwise euclidean distance matrix.\n",
    "    Returns:\n",
    "        triplet_loss: scalar tensor containing the triplet loss\n",
    "    \"\"\"\n",
    "    \n",
    "    # Get the pairwise distance matrix for L_itt\n",
    "    pairwise_dist = _pairwise_distances(image_embeddings, text_embeddings)\n",
    "    \n",
    "    # Get the pairwise distance matrix for L_tii\n",
    "    pairwise_dist_transpose = tf.transpose(pairwise_dist)\n",
    "\n",
    "    # shape (batch_size, batch_size, 1)\n",
    "    #Anchor Positive Dist is same for L_tii and L_itt\n",
    "    #identity_mask = tf.eye(tf.shape(pairwise_dist))\n",
    "    #anchor_positive_dist = tf.expand_dims(tf.multiply(identity_mask,pairwise_dist), 2)\n",
    "    anchor_positive_dist = tf.expand_dims(pairwise_dist, 2)\n",
    "    print(tf.shape(anchor_positive_dist))\n",
    "    #assert anchor_positive_dist.shape[2] == 1, \"{}\".format(anchor_positive_dist.shape)\n",
    "    \n",
    "    # shape (batch_size, 1, batch_size)\n",
    "    anchor_negative_dist = tf.expand_dims(pairwise_dist, 1) #L_itt\n",
    "    anchor_negative_dist_transpose = tf.expand_dims(pairwise_dist_transpose, 1) #L_tii\n",
    "    #assert anchor_negative_dist.shape[1] == 1, \"{}\".format(anchor_negative_dist.shape)\n",
    "    #assert anchor_negative_dist_transpose.shape[1] == 1, \"{}\".format(anchor_negative_dist_transpose.shape)\n",
    "\n",
    "    # Compute a 3D tensor of size (batch_size, batch_size, batch_size)\n",
    "    # triplet_loss[i, j, k] will contain the triplet loss of anchor=i, positive=j, negative=k\n",
    "    # Uses broadcasting where the 1st argument has shape (batch_size, batch_size, 1)\n",
    "    # and the 2nd (batch_size, 1, batch_size)\n",
    "    L_itt = anchor_positive_dist - anchor_negative_dist + margin\n",
    "    L_tii = anchor_positive_dist - anchor_negative_dist_transpose + margin\n",
    "\n",
    "    # Put to zero the invalid triplets\n",
    "    # (where label(a) != label(p) or label(n) == label(a) or a == p)\n",
    "    mask = _get_triplet_mask(labels)\n",
    "    mask = tf.to_float(mask)\n",
    "    L_itt = tf.multiply(mask, L_itt)\n",
    "    L_tii = tf.multiply(mask, L_tii)\n",
    "\n",
    "    # Remove negative losses (i.e. the easy triplets)\n",
    "    L_itt = tf.maximum(L_itt, 0.0)\n",
    "    L_tii = tf.maximum(L_tii, 0.0)\n",
    "    \n",
    "\n",
    "    # Count number of positive triplets (where triplet_loss > 0)\n",
    "    valid_triplets_tii = tf.to_float(tf.greater(L_tii, 1e-16))\n",
    "    valid_triplets_itt = tf.to_float(tf.greater(L_itt, 1e-16))\n",
    "    \n",
    "    num_positive_triplets_tii = tf.reduce_sum(valid_triplets_tii)\n",
    "    num_positive_triplets_itt = tf.reduce_sum(valid_triplets_itt)\n",
    "    \n",
    "    num_valid_triplets = tf.reduce_sum(mask)\n",
    "    fraction_positive_triplets = (num_positive_triplets_tii + num_positive_triplets_itt) / (2*num_valid_triplets + 1e-16)\n",
    "\n",
    "    # Get final mean triplet loss over the positive valid triplets\n",
    "    triplet_loss = (tf.reduce_sum(L_tii) + tf.reduce_sum(L_itt)) / (num_positive_triplets_tii + num_positive_triplets_itt + 1e-16)\n",
    "    \n",
    "    return triplet_loss, num_positive_triplets_tii, num_positive_triplets_itt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################################################################################\n",
    "\n",
    "#############################################################################################################\n",
    "\n",
    "###################################### tensorflow network ###################################################\n",
    "\n",
    "def image_net(image_dict, reuse, is_training):\n",
    "    with tf.variable_scope('ConvNet', reuse=reuse):\n",
    "        image_input= image_dict\n",
    "        image_input = tf.reshape(image_input, shape=[-1,512,512,3])\n",
    "        conv1 = tf.layers.conv2d(image_input, 32, 3, activation=tf.nn.relu)\n",
    "        conv1 = tf.layers.average_pooling2d(conv1,2,4)\n",
    "        conv2 = tf.layers.conv2d(conv1, 128, 3, activation = tf.nn.sigmoid)\n",
    "        conv2 = tf. layers.average_pooling2d(conv2,2,4)\n",
    "        conv3 = tf.layers.conv2d(conv2, 512, 3, activation = tf.nn.sigmoid)\n",
    "        conv3 = tf.layers.average_pooling2d(conv3,2,4)\n",
    "        flat = tf.contrib.layers.flatten(conv3)\n",
    "        tf.summary.histogram('Image_flatten', flat)\n",
    "        fc1 = tf.layers.dense(flat, 512, activation=None, name='fc1_dense')\n",
    "        out = tf.layers.batch_normalization(fc1, training=is_training,name='batchnorm')\n",
    "        tf.summary.histogram('Image_batch_norm_layer', out)\n",
    "        out = tf.identity(out, name ='my_feature_embedding')\n",
    "        tf.summary.histogram('Image_final_activation', out)\n",
    "        \n",
    "    return out \n",
    "\n",
    "def text_net (text_dict, reuse, is_training): \n",
    "    with tf.variable_scope('TextNet', reuse=reuse):\n",
    "        text_input = text_dict\n",
    "        fc1 = tf.layers.dense(text_input, 300, activation = tf.nn.sigmoid)\n",
    "        fc2 = tf.layers.dense(fc1, 400, activation = tf.nn.sigmoid)\n",
    "        fc3 = tf.layers.dense(fc2, 450, activation= tf.nn.sigmoid)\n",
    "        fc4 = tf.layers.dense(fc3, 512, activation = None)\n",
    "        tf.summary.histogram('Text_dense_4', fc4)\n",
    "        out = tf.layers.batch_normalization(fc4, training=is_training,name='text_batchnorm')\n",
    "        tf.summary.histogram('Text_batch_norm_layer', out)\n",
    "        out = tf.identity(out, name='my_text_embedding')\n",
    "        tf.summary.histogram('Text_final_activation', out)\n",
    "    return out\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "   \n",
    "###################################################################################################################\n",
    "tf.reset_default_graph()\n",
    "with tf.name_scope(\"inputs\"):\n",
    "    reuse = False\n",
    "    learning_rate = 0.001\n",
    "    image_dict = tf.placeholder(tf.float32, shape=(batch_size,512,512,3))\n",
    "    text_dict = tf.placeholder(tf.float32, shape = (batch_size,200))\n",
    "    labels = tf.placeholder(tf.float32, shape=(batch_size))\n",
    "    is_training = tf.placeholder_with_default(False, shape=[], name='training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Shape:0\", shape=(3,), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "####################################################################################################################\n",
    "\n",
    "image_embeddings = image_net(image_dict, reuse, is_training)\n",
    "text_embeddings = text_net(text_dict, reuse, is_training)\n",
    "image_embedding_mean_norm = tf.reduce_mean(tf.norm(image_embeddings, axis=1))\n",
    "tf.summary.scalar(\"embedding_image_mean_norm\", image_embedding_mean_norm)\n",
    "text_embedding_mean_norm = tf.reduce_mean(tf.norm(text_embeddings, axis=1))\n",
    "tf.summary.scalar(\"embedding_text_mean_norm\", text_embedding_mean_norm)\n",
    "# update operation to update batch-norm variables\n",
    "update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "with tf.control_dependencies(update_ops):\n",
    "    my_loss, num_tii, num_itt = batch_all_triplet_loss(labels, image_embeddings, text_embeddings, margin, squared=False)\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate, name='Adam-op')\n",
    "    minimizer = optimizer.minimize(my_loss)\n",
    "    \n",
    "    \n",
    "##########################################################################################################################\n",
    "\n",
    "with tf.name_scope(\"Init\"):\n",
    "    merged = tf.summary.merge_all()\n",
    "    init = tf.global_variables_initializer()\n",
    "    graph = tf.get_default_graph()\n",
    "    saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "##########################################################################################################################\n",
    "################################################# TRAINING SESSION ######################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing Session\n",
      "Init Done\n",
      "Running Batch Number 1\n",
      "1  -- Loss Val:  0.69343215  -- Trip_TII:  2450.0  -- Trip_ITT:  1758.0\n",
      "Running Batch Number 2\n",
      "2  -- Loss Val:  1.8151816  -- Trip_TII:  1734.0  -- Trip_ITT:  1252.0\n",
      "Running Batch Number 3\n",
      "3  -- Loss Val:  1.3507627  -- Trip_TII:  629.0  -- Trip_ITT:  934.0\n",
      "Running Batch Number 4\n",
      "4  -- Loss Val:  0.85602367  -- Trip_TII:  309.0  -- Trip_ITT:  515.0\n",
      "Running Batch Number 5\n",
      "5  -- Loss Val:  0.6237419  -- Trip_TII:  79.0  -- Trip_ITT:  340.0\n",
      "Running Batch Number 6\n",
      "6  -- Loss Val:  0.57980317  -- Trip_TII:  130.0  -- Trip_ITT:  235.0\n",
      "Running Batch Number 7\n",
      "7  -- Loss Val:  0.5386916  -- Trip_TII:  71.0  -- Trip_ITT:  85.0\n",
      "Running Batch Number 8\n",
      "8  -- Loss Val:  0.5319695  -- Trip_TII:  69.0  -- Trip_ITT:  83.0\n",
      "Running Batch Number 9\n",
      "9  -- Loss Val:  0.5597418  -- Trip_TII:  13.0  -- Trip_ITT:  15.0\n",
      "20  -- Loss Val:  0.27966687  -- Trip_TII:  15.0  -- Trip_ITT:  15.0\n",
      "40  -- Loss Val:  1.8118562  -- Trip_TII:  3.0  -- Trip_ITT:  10.0\n",
      "Model saved in path: ./models/model50.ckpt\n",
      "60  -- Loss Val:  0.38343963  -- Trip_TII:  2.0  -- Trip_ITT:  3.0\n",
      "80  -- Loss Val:  0.17308831  -- Trip_TII:  0.0  -- Trip_ITT:  2.0\n",
      "100  -- Loss Val:  1.2914507  -- Trip_TII:  5.0  -- Trip_ITT:  5.0\n",
      "Model saved in path: ./models/model100.ckpt\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    print(\"Initializing Session\")\n",
    "    sess.run(init)\n",
    "    print(\"Init Done\")\n",
    "    train_writer = tf.summary.FileWriter('./Mera_train/train',sess.graph)\n",
    "    test_writer = tf.summary.FileWriter('./Mera_test/test',sess.graph)\n",
    "    global_step = 0\n",
    "    count = 1\n",
    "    for [images, texts], labels_input in training_generator:\n",
    "        if count < 10:\n",
    "            print(\"Running Batch Number\", count)\n",
    "        feed_dict_batch = {image_dict: images, text_dict: texts, labels: labels_input,is_training: True}\n",
    "        loss_val, _num_tii, _num_itt, minimized = sess.run([my_loss, num_tii, num_itt, minimizer], feed_dict=feed_dict_batch)\n",
    "        if count%20 == 0 or count < 10:\n",
    "            print(count, \" -- Loss Val: \", loss_val, \" -- Trip_TII: \", _num_tii, \" -- Trip_ITT: \", _num_itt)\n",
    "            summary, _ = sess.run([merged,minimizer], feed_dict=feed_dict_batch)\n",
    "            train_writer.add_summary(summary, count)\n",
    "        if count%50 == 0:\n",
    "            save_path = saver.save(sess, \"./models/model\"+str(count)+\".ckpt\")\n",
    "            print(\"Model saved in path: %s\" % save_path)\n",
    "        if count%100 == 0:\n",
    "            break\n",
    "        count+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_image_data_flag = 1\n",
    "load_sig_data=0\n",
    "batch_size = 1\n",
    "count=500\n",
    "margin=0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_paths = []\n",
    "text_paths = []\n",
    "labels = []\n",
    "image_data=[]\n",
    "with open('./test_example.txt', 'r') as datafile:\n",
    "    for line in datafile:\n",
    "        [image_path, text_path, label] = line.split(\" \")\n",
    "        image_paths.append(image_path)\n",
    "        text_paths.append(text_path)\n",
    "        labels.append(label)\n",
    "for image in image_paths:\n",
    "            image_data.append(cv2.imread(image))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_data = np.array(image_data)\n",
    "\n",
    "labels = np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_embeddings=[]\n",
    "image_data1=image_data[:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_data1=labels[:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50, 512, 512, 3)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_data1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing Session\n",
      "INFO:tensorflow:Restoring parameters from ./models/model50.ckpt\n",
      "Model Restored\n",
      "Init Done\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "        print(\"Initializing Session\")\n",
    "        sess.run(init)\n",
    "        saver = tf.train.import_meta_graph('./models/model50.ckpt.meta')  # Put the meta file here\n",
    "        saver.restore(sess, \"./models/model50.ckpt\")\n",
    "#        saver.restore(sess, \"models/run_6/checkpoint\")\n",
    "        print(\"Model Restored\")\n",
    "        print(\"Init Done\")\n",
    "        #test_writer = tf.summary.FileWriter('./Graph_run6/test',sess.graph)\n",
    "        image_embeddings = graph.get_tensor_by_name(\"ConvNet/my_feature_embedding:0\")\n",
    "        image_dict = graph.get_tensor_by_name(\"inputs/Placeholder:0\") #same as above\n",
    "        \n",
    "        # Running for the Total_size/batch_size times\n",
    "        count=0\n",
    "        feed_dict_batch= {image_dict:image_data1 ,is_training:True}\n",
    "        test_embedding = sess.run(image_embeddings, feed_dict=feed_dict_batch)\n",
    "        test_embeddings.append(test_embedding)\n",
    "        print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_embeddings_np = np.array(test_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 0.12944031,  0.00280762, -0.08598328, ...,  0.03320312,\n",
       "          0.09320068, -0.2600708 ],\n",
       "        [-0.08644867, -0.08703613,  0.01959229, ..., -0.09265137,\n",
       "          0.11755371, -0.38360596],\n",
       "        [-0.28652954, -0.01855469,  0.31063843, ...,  0.10192871,\n",
       "         -0.27368164, -0.12030029],\n",
       "        ...,\n",
       "        [ 0.36410522, -0.62109375,  0.05033875, ..., -0.02185059,\n",
       "         -0.38079834, -0.2781372 ],\n",
       "        [ 0.38237762, -0.24707031, -0.22154236, ..., -0.23449707,\n",
       "         -0.26446533,  0.19818115],\n",
       "        [-0.48609924,  0.60424805, -0.82366943, ..., -0.07299805,\n",
       "          0.7598877 , -0.06854248]]], dtype=float32)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_embeddings_np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_emb_mean = np.mean(test_embeddings_np,axis=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.00702539, -0.01667348, -0.01516603, -0.00732607, -0.01955907,\n",
       "        -0.02634917, -0.02073777,  0.03263785,  0.02569321,  0.02530759,\n",
       "        -0.02510426, -0.02489255, -0.01473117,  0.01851849, -0.01894451,\n",
       "        -0.01249103,  0.00777126,  0.01938887,  0.01559644, -0.00565375,\n",
       "        -0.00407266,  0.00350026,  0.03871414, -0.01240822,  0.01022159,\n",
       "         0.01005187,  0.03707859, -0.02371478,  0.00238076, -0.01427337,\n",
       "        -0.01440587, -0.02118009,  0.01575859, -0.00063899, -0.01007861,\n",
       "         0.01574463,  0.00449814,  0.01721081,  0.00929052,  0.01651668,\n",
       "         0.01731256, -0.00186087, -0.00613387,  0.00584374,  0.01233236,\n",
       "        -0.0244056 , -0.00563774, -0.03469304,  0.01016186, -0.00342729]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_emb_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.03469304, -0.02634917, -0.02510426, -0.02489255, -0.0244056 ,\n",
       "       -0.02371478, -0.02118009, -0.02073777, -0.01955907, -0.01894451,\n",
       "       -0.01667348, -0.01516603, -0.01473117, -0.01440587, -0.01427337,\n",
       "       -0.01249103, -0.01240822, -0.01007861, -0.00732607, -0.00702539,\n",
       "       -0.00613387, -0.00565375, -0.00563774, -0.00407266, -0.00342729,\n",
       "       -0.00186087, -0.00063899,  0.00238076,  0.00350026,  0.00449814,\n",
       "        0.00584374,  0.00777126,  0.00929052,  0.01005187,  0.01016186,\n",
       "        0.01022159,  0.01233236,  0.01559644,  0.01574463,  0.01575859,\n",
       "        0.01651668,  0.01721081,  0.01731256,  0.01851849,  0.01938887,\n",
       "        0.02530759,  0.02569321,  0.03263785,  0.03707859,  0.03871414],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(test_emb_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    " test_embeddings_final=test_embeddings_np.reshape((50,512))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50, 512)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_embeddings_final.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50,)"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_data1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Solving environment: done\n",
      "\n",
      "## Package Plan ##\n",
      "\n",
      "  environment location: /home/ayush/anaconda3\n",
      "\n",
      "  added / updated specs: \n",
      "    - faiss-cpu\n",
      "\n",
      "\n",
      "The following packages will be downloaded:\n",
      "\n",
      "    package                    |            build\n",
      "    ---------------------------|-----------------\n",
      "    gstreamer-1.14.0           |       hb453b48_1         3.8 MB\n",
      "    fribidi-1.0.5              |       h7b6447c_0         112 KB\n",
      "    tk-8.6.8                   |       hbc83047_0         3.1 MB\n",
      "    cryptography-2.6.1         |   py36h1ba5d50_0         609 KB\n",
      "    libcurl-7.64.0             |       h20c2e04_2         600 KB\n",
      "    krb5-1.16.1                |       h173b8e3_7         1.4 MB\n",
      "    pcre-8.43                  |       he6710b0_0         260 KB\n",
      "    fontconfig-2.13.0          |       h9420a91_0         291 KB\n",
      "    pango-1.42.4               |       h049681c_0         528 KB\n",
      "    python-3.6.8               |       h0371630_0        34.4 MB\n",
      "    cairo-1.14.12              |       h8948797_3         1.3 MB\n",
      "    graphite2-1.3.13           |       h23475e2_0         101 KB\n",
      "    glib-2.56.2                |       hd408876_0         5.0 MB\n",
      "    libstdcxx-ng-8.2.0         |       hdf63c60_1         2.9 MB\n",
      "    libedit-3.1.20181209       |       hc058e9b_0         188 KB\n",
      "    dbus-1.13.2                |       h714fa37_1         554 KB\n",
      "    qt-5.6.3                   |       h8bf5577_3        45.7 MB\n",
      "    xz-5.2.4                   |       h14c3975_4         366 KB\n",
      "    conda-4.6.12               |           py36_1         2.1 MB\n",
      "    libssh2-1.8.0              |       h1ba5d50_4         233 KB\n",
      "    freetype-2.9.1             |       h8a8886c_1         822 KB\n",
      "    numpy-1.14.2               |   py36hdbf6ddf_0         4.0 MB\n",
      "    faiss-cpu-1.5.1            |   py36h6bb024c_1         904 KB  pytorch\n",
      "    libpng-1.6.36              |       hbc83047_0         346 KB\n",
      "    ncurses-6.1                |       he6710b0_1         958 KB\n",
      "    pillow-5.4.1               |   py36h34e0f95_0         627 KB\n",
      "    pycurl-7.43.0.2            |   py36h1ba5d50_0         185 KB\n",
      "    certifi-2019.3.9           |           py36_0         155 KB\n",
      "    libgcc-ng-8.2.0            |       hdf63c60_1         7.6 MB\n",
      "    libuuid-1.0.3              |       h1bed415_2          16 KB\n",
      "    libxcb-1.13                |       h1bed415_1         502 KB\n",
      "    gst-plugins-base-1.14.0    |       hbbd80ab_1         6.3 MB\n",
      "    ca-certificates-2019.1.23  |                0         126 KB\n",
      "    openssl-1.1.1b             |       h7b6447c_1         4.0 MB\n",
      "    libxml2-2.9.9              |       he19cac6_0         2.0 MB\n",
      "    curl-7.64.0                |       hbc83047_2         152 KB\n",
      "    harfbuzz-1.8.8             |       hffaf4a1_0         863 KB\n",
      "    sqlite-3.27.2              |       h7b6447c_0         1.9 MB\n",
      "    blas-1.0                   |              mkl           6 KB\n",
      "    ------------------------------------------------------------\n",
      "                                           Total:       134.8 MB\n",
      "\n",
      "The following NEW packages will be INSTALLED:\n",
      "\n",
      "    blas:             1.0-mkl                        \n",
      "    faiss-cpu:        1.5.1-py36h6bb024c_1    pytorch\n",
      "    fribidi:          1.0.5-h7b6447c_0               \n",
      "    krb5:             1.16.1-h173b8e3_7              \n",
      "    libuuid:          1.0.3-h1bed415_2               \n",
      "\n",
      "The following packages will be UPDATED:\n",
      "\n",
      "    ca-certificates:  2017.08.26-h1d4fec5_0           --> 2019.1.23-0            \n",
      "    cairo:            1.14.12-h77bcde2_0              --> 1.14.12-h8948797_3     \n",
      "    certifi:          2018.1.18-py36_0                --> 2019.3.9-py36_0        \n",
      "    conda:            4.5.1-py36_0                    --> 4.6.12-py36_1          \n",
      "    cryptography:     2.1.4-py36hd09be54_0            --> 2.6.1-py36h1ba5d50_0   \n",
      "    curl:             7.58.0-h84994c4_0               --> 7.64.0-hbc83047_2      \n",
      "    dbus:             1.12.2-hc3f9b76_1               --> 1.13.2-h714fa37_1      \n",
      "    fontconfig:       2.12.4-h88586e7_1               --> 2.13.0-h9420a91_0      \n",
      "    freetype:         2.8-hab7d2ae_1                  --> 2.9.1-h8a8886c_1       \n",
      "    glib:             2.53.6-h5d9569c_2               --> 2.56.2-hd408876_0      \n",
      "    graphite2:        1.3.10-hf63cedd_1               --> 1.3.13-h23475e2_0      \n",
      "    gst-plugins-base: 1.12.4-h33fb286_0               --> 1.14.0-hbbd80ab_1      \n",
      "    gstreamer:        1.12.4-hb53b477_0               --> 1.14.0-hb453b48_1      \n",
      "    harfbuzz:         1.7.4-hc5b324e_0                --> 1.8.8-hffaf4a1_0       \n",
      "    libcurl:          7.58.0-h1ad7b7a_0               --> 7.64.0-h20c2e04_2      \n",
      "    libedit:          3.1-heed3624_0                  --> 3.1.20181209-hc058e9b_0\n",
      "    libgcc-ng:        7.2.0-h7cc24e2_2                --> 8.2.0-hdf63c60_1       \n",
      "    libpng:           1.6.34-hb9fc6fc_0               --> 1.6.36-hbc83047_0      \n",
      "    libssh2:          1.8.0-h9cfc8f7_4                --> 1.8.0-h1ba5d50_4       \n",
      "    libstdcxx-ng:     7.2.0-h7a57d05_2                --> 8.2.0-hdf63c60_1       \n",
      "    libxcb:           1.12-hcd93eb1_4                 --> 1.13-h1bed415_1        \n",
      "    libxml2:          2.9.7-h26e45fe_0                --> 2.9.9-he19cac6_0       \n",
      "    ncurses:          6.0-h9df7e31_2                  --> 6.1-he6710b0_1         \n",
      "    numpy:            1.14.0-py36h3dfced4_1           --> 1.14.2-py36hdbf6ddf_0  \n",
      "    openssl:          1.0.2n-hb7f436b_0               --> 1.1.1b-h7b6447c_1      \n",
      "    pango:            1.41.0-hd475d92_0               --> 1.42.4-h049681c_0      \n",
      "    pcre:             8.41-hc27e229_1                 --> 8.43-he6710b0_0        \n",
      "    pillow:           5.0.0-py36h3deb7b8_0            --> 5.4.1-py36h34e0f95_0   \n",
      "    pycurl:           7.43.0.1-py36hb7f436b_0         --> 7.43.0.2-py36h1ba5d50_0\n",
      "    python:           3.6.4-hc3d631a_1                --> 3.6.8-h0371630_0       \n",
      "    qt:               5.6.2-h974d657_12               --> 5.6.3-h8bf5577_3       \n",
      "    sqlite:           3.22.0-h1bed415_0               --> 3.27.2-h7b6447c_0      \n",
      "    tk:               8.6.7-hc745277_3                --> 8.6.8-hbc83047_0       \n",
      "    xz:               5.2.3-h55aa19d_2                --> 5.2.4-h14c3975_4       \n",
      "\n",
      "\n",
      "Downloading and Extracting Packages\n",
      "gstreamer 1.14.0######################################################## | 100% \n",
      "fribidi 1.0.5########################################################### | 100% \n",
      "tk 8.6.8################################################################ | 100% \n",
      "cryptography 2.6.1###################################################### | 100% \n",
      "libcurl 7.64.0########################################################## | 100% \n",
      "krb5 1.16.1############################################################# | 100% \n",
      "pcre 8.43############################################################### | 100% \n",
      "fontconfig 2.13.0####################################################### | 100% \n",
      "pango 1.42.4############################################################ | 100% \n",
      "python 3.6.8############################################################ | 100% \n",
      "cairo 1.14.12########################################################### | 100% \n",
      "graphite2 1.3.13######################################################## | 100% \n",
      "glib 2.56.2############################################################# | 100% \n",
      "libstdcxx-ng 8.2.0###################################################### | 100% \n",
      "libedit 3.1.20181209#################################################### | 100% \n",
      "dbus 1.13.2############################################################# | 100% \n",
      "qt 5.6.3################################################################ | 100% \n",
      "xz 5.2.4################################################################ | 100% \n",
      "conda 4.6.12############################################################ | 100% \n",
      "libssh2 1.8.0########################################################### | 100% \n",
      "freetype 2.9.1########################################################## | 100% \n",
      "numpy 1.14.2############################################################ | 100% \n",
      "faiss-cpu 1.5.1######################################################### | 100% \n",
      "libpng 1.6.36########################################################### | 100% \n",
      "ncurses 6.1############################################################# | 100% \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pillow 5.4.1############################################################ | 100% \n",
      "pycurl 7.43.0.2######################################################### | 100% \n",
      "certifi 2019.3.9######################################################## | 100% \n",
      "libgcc-ng 8.2.0######################################################### | 100% \n",
      "libuuid 1.0.3########################################################### | 100% \n",
      "libxcb 1.13############################################################# | 100% \n",
      "gst-plugins-base 1.14.0################################################# | 100% \n",
      "ca-certificates 2019.1.23############################################### | 100% \n",
      "openssl 1.1.1b########################################################## | 100% \n",
      "libxml2 2.9.9########################################################### | 100% \n",
      "curl 7.64.0############################################################# | 100% \n",
      "harfbuzz 1.8.8########################################################## | 100% \n",
      "sqlite 3.27.2########################################################### | 100% \n",
      "blas 1.0################################################################ | 100% \n",
      "Preparing transaction: done\n",
      "Verifying transaction: done\n",
      "Executing transaction: done\n"
     ]
    }
   ],
   "source": [
    "!conda install --yes faiss-cpu -c pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "50\n",
      "Doing Sanity Check\n",
      "[[ 0]\n",
      " [ 1]\n",
      " [ 2]\n",
      " [ 3]\n",
      " [ 4]\n",
      " [ 5]\n",
      " [ 6]\n",
      " [ 7]\n",
      " [ 8]\n",
      " [ 9]\n",
      " [10]\n",
      " [11]\n",
      " [12]\n",
      " [13]\n",
      " [14]\n",
      " [15]\n",
      " [16]\n",
      " [17]\n",
      " [18]\n",
      " [19]\n",
      " [20]\n",
      " [21]\n",
      " [22]\n",
      " [23]\n",
      " [24]\n",
      " [25]\n",
      " [26]\n",
      " [27]\n",
      " [28]\n",
      " [29]\n",
      " [30]\n",
      " [31]\n",
      " [32]\n",
      " [33]\n",
      " [34]\n",
      " [35]\n",
      " [36]\n",
      " [37]\n",
      " [38]\n",
      " [39]\n",
      " [40]\n",
      " [41]\n",
      " [42]\n",
      " [43]\n",
      " [44]\n",
      " [45]\n",
      " [46]\n",
      " [47]\n",
      " [48]\n",
      " [49]]\n",
      "[[0.0000000e+00]\n",
      " [0.0000000e+00]\n",
      " [1.5258789e-05]\n",
      " [0.0000000e+00]\n",
      " [0.0000000e+00]\n",
      " [0.0000000e+00]\n",
      " [0.0000000e+00]\n",
      " [1.5258789e-05]\n",
      " [0.0000000e+00]\n",
      " [0.0000000e+00]\n",
      " [0.0000000e+00]\n",
      " [0.0000000e+00]\n",
      " [0.0000000e+00]\n",
      " [7.6293945e-06]\n",
      " [0.0000000e+00]\n",
      " [0.0000000e+00]\n",
      " [0.0000000e+00]\n",
      " [0.0000000e+00]\n",
      " [0.0000000e+00]\n",
      " [0.0000000e+00]\n",
      " [0.0000000e+00]\n",
      " [0.0000000e+00]\n",
      " [0.0000000e+00]\n",
      " [0.0000000e+00]\n",
      " [0.0000000e+00]\n",
      " [0.0000000e+00]\n",
      " [7.6293945e-06]\n",
      " [0.0000000e+00]\n",
      " [0.0000000e+00]\n",
      " [1.5258789e-05]\n",
      " [0.0000000e+00]\n",
      " [7.6293945e-06]\n",
      " [0.0000000e+00]\n",
      " [0.0000000e+00]\n",
      " [0.0000000e+00]\n",
      " [0.0000000e+00]\n",
      " [0.0000000e+00]\n",
      " [0.0000000e+00]\n",
      " [0.0000000e+00]\n",
      " [0.0000000e+00]\n",
      " [7.6293945e-06]\n",
      " [7.6293945e-06]\n",
      " [0.0000000e+00]\n",
      " [2.2888184e-05]\n",
      " [0.0000000e+00]\n",
      " [0.0000000e+00]\n",
      " [0.0000000e+00]\n",
      " [0.0000000e+00]\n",
      " [0.0000000e+00]\n",
      " [0.0000000e+00]]\n"
     ]
    }
   ],
   "source": [
    "import faiss\n",
    "import sys\n",
    "d=512\n",
    "nb = test_embeddings_final.shape[0]\n",
    "nq = labels_data1.shape[0]\n",
    "index = faiss.IndexFlatL2(d)\n",
    "print(index.is_trained)\n",
    "sys.stdout.flush()\n",
    "index.add(test_embeddings_final)                  # add vectors to the index\n",
    "print(index.ntotal)\n",
    "sys.stdout.flush()\n",
    "k = 1\n",
    "print(\"Doing Sanity Check\")\n",
    "sys.stdout.flush()\n",
    "D, I = index.search(test_embeddings_final, k) # sanity check\n",
    "print(I)\n",
    "print(D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
