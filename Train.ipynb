{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'keras'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-a6a3d7bd5b1f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfunctools\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'keras'"
     ]
    }
   ],
   "source": [
    "#Import libs\n",
    "import time\n",
    "import os, os.path\n",
    "import random\n",
    "import cv2\n",
    "import keras\n",
    "import matplotlib\n",
    "import functools\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.cluster import KMeans\n",
    "import tensorflow as tf\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.decomposition import PCA\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten, Input, Conv2D, MaxPooling2D, Concatenate, GlobalAveragePooling2D, BatchNormalization\n",
    "from keras.initializers import glorot_normal\n",
    "from keras import optimizers\n",
    "import keras.backend as K\n",
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import euclidean_distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Hyperparameters\n",
    "\n",
    "BatchSize = 64\n",
    "Alpha = 0.5\n",
    "\n",
    "#Read Data Paths\n",
    "\n",
    "images = sorted(glob.glob('/work/cvma/FP/data/*/*[!_xyt]/*'))\n",
    "xyt = sorted(glob.glob('/work/cvma/FP/data/*/*_xyt/*'))\n",
    "image_xyt_pairs = [list(a) for a in zip(images,xyt)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image List:  0 Text List :  0 Labels :  0\n"
     ]
    }
   ],
   "source": [
    "#Image Data Preprocessing\n",
    "\n",
    "label_dict = {}\n",
    "label_counter = 0\n",
    "labels = []\n",
    "image_list = []\n",
    "text_list = []\n",
    "\n",
    "#Minutae Data Preprocessing\n",
    "def crop_top(df,crop_size):\n",
    "    #this will take top n inputs for df\n",
    "    if df.shape[0]<crop_size:\n",
    "        d_zero = np.zeros((crop_size,df.shape[1]), dtype=int)\n",
    "        d_zero[:df.shape[0], :df.shape[1]]=df\n",
    "        return d_zero\n",
    "    else:\n",
    "        return df[0:crop_size,:]\n",
    "    \n",
    "def preprocess_xyt_file(filename, crop_size=50):\n",
    "    df=pd.read_csv(filename, sep=' ',header=None)\n",
    "    df = np.array(df)\n",
    "    np.sort(df, axis=0)\n",
    "    df = df[df[:,3].argsort()[::-1]]\n",
    "\n",
    "    normalization_factor = np.array([512,512,360,100])\n",
    "    df_new = crop_top(df,crop_size)/normalization_factor\n",
    "    \n",
    "    return df_new.flatten()\n",
    "\n",
    "def assign_label(key):\n",
    "    global label_counter\n",
    "    if key in label_dict.keys():\n",
    "        labels.append(label_dict[key])\n",
    "        return 0\n",
    "    else:\n",
    "        label_dict[key] = label_counter\n",
    "        labels.append(label_counter)\n",
    "        label_counter=label_counter+1\n",
    "        #Do Something\n",
    "        return 0\n",
    "    \n",
    "for image in images:\n",
    "    filename = image.split(\"/\")[-1]\n",
    "    foldername = \"/\".join(image.split(\"/\")[0:-1])\n",
    "    tags = filename.split(\"_\")\n",
    "    if len(tags) == 5:\n",
    "        key = foldername+\"/\"+\"_\".join(tags[0:4])\n",
    "        assign_label(key)\n",
    "        image_data = cv2.imread(image)\n",
    "        image_list.append(image_data)\n",
    "        Index = images.index(image)\n",
    "        text_list.append(preprocess_xyt_file(xyt[Index]))\n",
    "\n",
    "    elif len(tags) == 3:\n",
    "        key = foldername+\"/\"+tags[0]+\"_\"+tags[2]\n",
    "        assign_label(key)\n",
    "        image_data = cv2.imread(image)\n",
    "        image_list.append(image_data)\n",
    "        Index = images.index(image)\n",
    "        text_list.append(preprocess_xyt_file(xyt[Index]))\n",
    "        #elif len(tags) == 4:\n",
    "        \n",
    "    #elif len(tags) == 2:\n",
    "        \n",
    "print(\"Image List: \", len(image_list), \"Text List : \", len(text_list), \"Labels : \", len(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"Define functions to create the triplet loss with online triplet mining.\"\"\"\n",
    "\n",
    "ses = tf.Session()\n",
    "\n",
    "image_embeddings = tf.random_uniform([1000,512], minval=0, maxval=1, dtype=tf.float32)\n",
    "image_embeddings = tf.constant([[1.0,1.0,1.0],[3.0,3.0,3.0]])\n",
    "text_embeddings = tf.random_uniform([1000,512], minval=0, maxval=1, dtype=tf.float32)\n",
    "text_embeddings = tf.constant([[1.0,2.0,1.0],[1.0,2.0,3.0]])\n",
    "y_pred = tf.concat([image_embeddings, text_embeddings], 0)\n",
    "\n",
    "y1,y2 =tf.split(y_pred, num_or_size_splits=2, axis=0)\n",
    "tf.shape(image_embeddings)\n",
    "\n",
    "def euclid(image,text):\n",
    "    return euclidean_distances(image,text,squared=True)\n",
    "\n",
    "def _pairwise_distances(image_embeddings, text_embeddings, squared=True):\n",
    "    #Returns Pairwise Euclidean Distances\n",
    "    if squared == True:\n",
    "        distances = tf.py_func(euclid, [image_embeddings, text_embeddings], tf.float32)\n",
    "    else:\n",
    "        distances = tf.py_func(euclidean_distances,[image_embeddings, text_embeddings], tf.float32)        \n",
    "    return distances\n",
    "\n",
    "#No need for positive mask in Multimodal Triplet. Anchor and Positives will be fed through data inputs.\n",
    "\n",
    "def _get_anchor_negative_triplet_mask(labels):\n",
    "    \"\"\"Return a 2D mask where mask[a, n] is True iff a and n have distinct labels.\n",
    "\n",
    "    Args:\n",
    "        labels: tf.int32 `Tensor` with shape [batch_size]\n",
    "\n",
    "    Returns:\n",
    "        mask: tf.bool `Tensor` with shape [batch_size, batch_size]\n",
    "    \"\"\"\n",
    "    # Check if labels[i] != labels[k]\n",
    "    # Uses broadcasting where the 1st argument has shape (1, batch_size) and the 2nd (batch_size, 1)\n",
    "    labels_equal = tf.equal(tf.expand_dims(labels, 0), tf.expand_dims(labels, 1))\n",
    "\n",
    "    mask = tf.logical_not(labels_equal)\n",
    "\n",
    "    return mask\n",
    "\n",
    "\n",
    "def _get_triplet_mask(labels):\n",
    "    \"\"\"Return a 3D mask where mask[a, p, n] is True iff the triplet (a, p, n) is valid.\n",
    "\n",
    "    A triplet (i, j, k) is valid if:\n",
    "        - i == j, k is distinct\n",
    "        - i == j and labels[i] != labels[k]\n",
    "\n",
    "    Args:\n",
    "        labels: tf.int32 `Tensor` with shape [batch_size]\n",
    "    \"\"\"\n",
    "    # Check that i == j and k is distinct\n",
    "    indices_equal = tf.cast(tf.eye(tf.shape(labels)[0]), tf.bool)\n",
    "    indices_not_equal = tf.logical_not(indices_equal)\n",
    "    i_equal_j = tf.expand_dims(indices_equal, 2)\n",
    "    i_not_equal_k = tf.expand_dims(indices_not_equal, 1)\n",
    "    j_not_equal_k = tf.expand_dims(indices_not_equal, 0)\n",
    "\n",
    "    valid_indices = tf.logical_and(tf.logical_and(i_equal_j, i_not_equal_k), j_not_equal_k)\n",
    "\n",
    "\n",
    "    # Check if labels[i] == labels[j] and labels[i] != labels[k]\n",
    "    label_equal = tf.equal(tf.expand_dims(labels, 0), tf.expand_dims(labels, 1))\n",
    "    i_equal_j = tf.expand_dims(label_equal, 2)\n",
    "    i_equal_k = tf.expand_dims(label_equal, 1)\n",
    "\n",
    "    valid_labels = tf.logical_and(i_equal_j, tf.logical_not(i_equal_k))\n",
    "\n",
    "    # Combine the two masks\n",
    "    mask = tf.logical_and(valid_indices, valid_labels)\n",
    "\n",
    "    return mask\n",
    "\n",
    "#No need for triplet mask. Negative Mask, serves as triplet mask for both L_tii and L_itt.\n",
    "\n",
    "def batch_all_triplet_loss(labels, image_embeddings, text_embeddings, margin, squared=False):\n",
    "    \"\"\"Build the triplet loss over a batch of embeddings.\n",
    "\n",
    "    We generate all the valid triplets and average the loss over the positive ones.\n",
    "\n",
    "    Args:\n",
    "        labels: labels of the batch, of size (batch_size,)\n",
    "        embeddings: tensor of shape (batch_size, embed_dim)\n",
    "        margin: margin for triplet loss\n",
    "        squared: Boolean. If true, output is the pairwise squared euclidean distance matrix.\n",
    "                 If false, output is the pairwise euclidean distance matrix.\n",
    "\n",
    "    Returns:\n",
    "        triplet_loss: scalar tensor containing the triplet loss\n",
    "    \"\"\"\n",
    "    \n",
    "    # Get the pairwise distance matrix for L_itt\n",
    "    pairwise_dist = _pairwise_distances(image_embeddings, text_embeddings)\n",
    "    \n",
    "    # Get the pairwise distance matrix for L_tii\n",
    "    pairwise_dist_transpose = tf.transpose(pairwise_dist)\n",
    "\n",
    "    # shape (batch_size, batch_size, 1)\n",
    "    #Anchor Positive Dist is same for L_tii and L_itt\n",
    "    #identity_mask = tf.eye(tf.shape(pairwise_dist))\n",
    "    #anchor_positive_dist = tf.expand_dims(tf.multiply(identity_mask,pairwise_dist), 2)\n",
    "    anchor_positive_dist = tf.expand_dims(pairwise_dist, 2)\n",
    "    print(tf.shape(anchor_positive_dist))\n",
    "    #assert anchor_positive_dist.shape[2] == 1, \"{}\".format(anchor_positive_dist.shape)\n",
    "    \n",
    "    # shape (batch_size, 1, batch_size)\n",
    "    anchor_negative_dist = tf.expand_dims(pairwise_dist, 1) #L_itt\n",
    "    anchor_negative_dist_transpose = tf.expand_dims(pairwise_dist_transpose, 1) #L_tii\n",
    "    #assert anchor_negative_dist.shape[1] == 1, \"{}\".format(anchor_negative_dist.shape)\n",
    "    #assert anchor_negative_dist_transpose.shape[1] == 1, \"{}\".format(anchor_negative_dist_transpose.shape)\n",
    "\n",
    "    # Compute a 3D tensor of size (batch_size, batch_size, batch_size)\n",
    "    # triplet_loss[i, j, k] will contain the triplet loss of anchor=i, positive=j, negative=k\n",
    "    # Uses broadcasting where the 1st argument has shape (batch_size, batch_size, 1)\n",
    "    # and the 2nd (batch_size, 1, batch_size)\n",
    "    L_itt = anchor_positive_dist - anchor_negative_dist + margin\n",
    "    L_tii = anchor_positive_dist - anchor_negative_dist_transpose + margin\n",
    "\n",
    "    # Put to zero the invalid triplets\n",
    "    # (where label(a) != label(p) or label(n) == label(a) or a == p)\n",
    "    mask = _get_triplet_mask(labels)\n",
    "    mask = tf.to_float(mask)\n",
    "    L_itt = tf.multiply(mask, L_itt)\n",
    "    L_tii = tf.multiply(mask, L_tii)\n",
    "\n",
    "    # Remove negative losses (i.e. the easy triplets)\n",
    "    L_itt = tf.maximum(L_itt, 0.0)\n",
    "    L_tii = tf.maximum(L_tii, 0.0)\n",
    "    \n",
    "\n",
    "    # Count number of positive triplets (where triplet_loss > 0)\n",
    "    valid_triplets_tii = tf.to_float(tf.greater(L_tii, 1e-16))\n",
    "    valid_triplets_itt = tf.to_float(tf.greater(L_itt, 1e-16))\n",
    "    \n",
    "    num_positive_triplets_tii = tf.reduce_sum(valid_triplets_tii)\n",
    "    num_positive_triplets_itt = tf.reduce_sum(valid_triplets_itt)\n",
    "    \n",
    "    num_valid_triplets = tf.reduce_sum(mask)\n",
    "    fraction_positive_triplets = (num_positive_triplets_tii + num_positive_triplets_itt) / (2*num_valid_triplets + 1e-16)\n",
    "\n",
    "    # Get final mean triplet loss over the positive valid triplets\n",
    "    triplet_loss = (tf.reduce_sum(L_tii) + tf.reduce_sum(L_itt)) / (num_positive_triplets_tii + num_positive_triplets_itt + 1e-16)\n",
    "    \n",
    "    return triplet_loss\n",
    "\n",
    "\n",
    "def batch_hard_triplet_loss(labels, embeddings, margin, squared=False):\n",
    "    \"\"\"Build the triplet loss over a batch of embeddings.\n",
    "\n",
    "    For each anchor, we get the hardest positive and hardest negative to form a triplet.\n",
    "\n",
    "    Args:\n",
    "        labels: labels of the batch, of size (batch_size,)\n",
    "        embeddings: tensor of shape (batch_size, embed_dim)\n",
    "        margin: margin for triplet loss\n",
    "        squared: Boolean. If true, output is the pairwise squared euclidean distance matrix.\n",
    "                 If false, output is the pairwise euclidean distance matrix.\n",
    "\n",
    "    Returns:\n",
    "        triplet_loss: scalar tensor containing the triplet loss\n",
    "    \"\"\"\n",
    "    # Get the pairwise distance matrix\n",
    "    pairwise_dist = _pairwise_distances(embeddings, squared=squared)\n",
    "\n",
    "    # For each anchor, get the hardest positive\n",
    "    # First, we need to get a mask for every valid positive (they should have same label)\n",
    "    mask_anchor_positive = _get_anchor_positive_triplet_mask(labels)\n",
    "    mask_anchor_positive = tf.to_float(mask_anchor_positive)\n",
    "\n",
    "    # We put to 0 any element where (a, p) is not valid (valid if a != p and label(a) == label(p))\n",
    "    anchor_positive_dist = tf.multiply(mask_anchor_positive, pairwise_dist)\n",
    "\n",
    "    # shape (batch_size, 1)\n",
    "    hardest_positive_dist = tf.reduce_max(anchor_positive_dist, axis=1, keepdims=True)\n",
    "    tf.summary.scalar(\"hardest_positive_dist\", tf.reduce_mean(hardest_positive_dist))\n",
    "\n",
    "    # For each anchor, get the hardest negative\n",
    "    # First, we need to get a mask for every valid negative (they should have different labels)\n",
    "    mask_anchor_negative = _get_anchor_negative_triplet_mask(labels)\n",
    "    mask_anchor_negative = tf.to_float(mask_anchor_negative)\n",
    "\n",
    "    # We add the maximum value in each row to the invalid negatives (label(a) == label(n))\n",
    "    max_anchor_negative_dist = tf.reduce_max(pairwise_dist, axis=1, keepdims=True)\n",
    "    anchor_negative_dist = pairwise_dist + max_anchor_negative_dist * (1.0 - mask_anchor_negative)\n",
    "\n",
    "    # shape (batch_size,)\n",
    "    hardest_negative_dist = tf.reduce_min(anchor_negative_dist, axis=1, keepdims=True)\n",
    "    tf.summary.scalar(\"hardest_negative_dist\", tf.reduce_mean(hardest_negative_dist))\n",
    "\n",
    "    # Combine biggest d(a, p) and smallest d(a, n) into final triplet loss\n",
    "    triplet_loss = tf.maximum(hardest_positive_dist - hardest_negative_dist + margin, 0.0)\n",
    "\n",
    "    # Get final mean triplet loss\n",
    "    triplet_loss = tf.reduce_mean(triplet_loss)\n",
    "\n",
    "    return triplet_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ayush/anaconda3/lib/python3.6/site-packages/keras_applications/resnet50.py:265: UserWarning: The output shape of `ResNet50(include_top=False)` has been changed since Keras 2.2.0.\n",
      "  warnings.warn('The output shape of `ResNet50(include_top=False)` '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"loss_1/concatenate_2_loss/Shape:0\", shape=(?,), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "input_images = Input(shape=(512,512,3))\n",
    "resnet50_model = keras.applications.resnet50.ResNet50(include_top=False, weights=\"imagenet\", input_shape=(512,512,3))\n",
    "x = resnet50_model(input_images)\n",
    "x = GlobalAveragePooling2D(data_format='channels_last')(x)\n",
    "x = Dense(1024,kernel_initializer=glorot_normal(seed=None), activation='sigmoid')(x)\n",
    "x = Dense(512, kernel_initializer=glorot_normal(seed=None), activation=None)(x)\n",
    "output_image = BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001, center=True, scale=True, beta_initializer='zeros', gamma_initializer='ones', moving_mean_initializer='zeros', moving_variance_initializer='ones', beta_regularizer=None, gamma_regularizer=None, beta_constraint=None, gamma_constraint=None)(x)\n",
    "#x.summary()\n",
    "\n",
    "input_text = Input(shape=(200,))\n",
    "y = Dense(400, kernel_initializer=glorot_normal(seed=None), activation='sigmoid', input_shape = (200,))(input_text)\n",
    "y = Dense(512, kernel_initializer=glorot_normal(seed=None), activation = None)(y)\n",
    "output_text = BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001, center=True, scale=True, beta_initializer='zeros', gamma_initializer='ones', moving_mean_initializer='zeros', moving_variance_initializer='ones', beta_regularizer=None, gamma_regularizer=None, beta_constraint=None, gamma_constraint=None)(y)\n",
    "#y.summary()\n",
    "\n",
    "#model_combined = Sequential()\n",
    "mergedOut = Concatenate(axis=0)([output_image,output_text])\n",
    "model_combined=Model(inputs=[input_images, input_text], outputs=mergedOut)\n",
    "\n",
    "labels=[0,1]\n",
    "y_true = labels\n",
    "embeddings = tf.concat([image_embeddings, text_embeddings], 0)\n",
    "\n",
    "#def triplet_loss(image_embeddings, text_embeddings, margin, squared=True):\n",
    "def _batch_all_triplet_loss(labels,image_embeddings, text_embeddings, margin):\n",
    "    return batch_all_triplet_loss(labels,image_embeddings, text_embeddings, margin, squared=True)\n",
    "\n",
    "def triplet_loss(margin):\n",
    "    @functools.wraps(_batch_all_triplet_loss)\n",
    "    def loss(labels, embeddings):\n",
    "        image_embeddings, text_embeddings =tf.split(embeddings, num_or_size_splits=2, axis=0)\n",
    "        return _batch_all_triplet_loss(labels,image_embeddings, text_embeddings, margin)\n",
    "    return loss\n",
    "\n",
    "triplet_loss = triplet_loss(Alpha)\n",
    "sgd = optimizers.SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "model_combined.compile(loss=triplet_loss, optimizer=sgd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Shape_5:0\", shape=(?,), dtype=int32)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'truediv_3:0' shape=() dtype=float32>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "triplet_loss([0,1],image_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Image Data Preprocessing\n",
    "\n",
    "label_dict = {}\n",
    "label_counter = 0\n",
    "label = []\n",
    "\n",
    "def assign_label(key):\n",
    "    global label_counter\n",
    "    if key in label_dict.keys():\n",
    "        label.append(label_dict[key])\n",
    "        return 0\n",
    "    else:\n",
    "        label_dict[key] = label_counter\n",
    "        label.append(label_counter)\n",
    "        label_counter=label_counter+1\n",
    "        #Do Something\n",
    "        return 0\n",
    "for image in images:\n",
    "    filename = image.split(\"/\")[-1]\n",
    "    foldername = \"/\".join(image.split(\"/\")[0:-1])\n",
    "    tags = filename.split(\"_\")\n",
    "    if len(tags) == 5:\n",
    "        key = foldername+\"/\"+\"_\".join(tags[0:4])\n",
    "        assign_label(key)\n",
    "\n",
    "    elif len(tags) == 3:\n",
    "        key = foldername+\"/\"+tags[0]+\"_\"+tags[2]\n",
    "        assign_label(key)\n",
    "    #elif len(tags) == 4:\n",
    "        \n",
    "    #elif len(tags) == 2:\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Minutae Data Preprocessing\n",
    "def crop_top(df,crop_size):\n",
    "    #this will take top n inputs for df\n",
    "    if df.shape[0]<crop_size:\n",
    "        d_zero = np.zeros((crop_size,df.shape[1]), dtype=int)\n",
    "        d_zero[:df.shape[0], :df.shape[1]]=df\n",
    "        return d_zero\n",
    "    else:\n",
    "        return df[0:crop_size,:]\n",
    "    \n",
    "def preprocess_xyt_file(filename):\n",
    "    df=pd.read_csv(filename, sep=' ',header=None)\n",
    "    df = np.array(df)\n",
    "    np.sort(df, axis=0)\n",
    "    df = df[df[:,3].argsort()[::-1]]\n",
    "\n",
    "    normalization_factor = np.array([512,512,360,100])\n",
    "    df_new = crop_top(df,50)/normalization_factor\n",
    "    \n",
    "    return df_new.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"Define functions to create the triplet loss with online triplet mining.\"\"\"\n",
    "\n",
    "ses = tf.Session()\n",
    "\n",
    "image_embeddings = tf.random_uniform([1000,512], minval=0, maxval=1, dtype=tf.float32)\n",
    "image_embeddings = tf.constant([[1.0,1.0,1.0],[3.0,3.0,3.0]])\n",
    "text_embeddings = tf.random_uniform([1000,512], minval=0, maxval=1, dtype=tf.float32)\n",
    "text_embeddings = tf.constant([[1.0,2.0,1.0],[1.0,2.0,3.0]])\n",
    "y_pred = tf.concat([image_embeddings, text_embeddings], 0)\n",
    "\n",
    "y1,y2 =tf.split(y_pred, num_or_size_splits=2, axis=0)\n",
    "tf.shape(image_embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def euclid(image,text):\n",
    "    return euclidean_distances(image,text,squared=True)\n",
    "\n",
    "def _pairwise_distances(image_embeddings, text_embeddings, squared=True):\n",
    "    #Returns Pairwise Euclidean Distances\n",
    "    if squared == True:\n",
    "        distances = tf.py_func(euclid, [image_embeddings, text_embeddings], tf.float32)\n",
    "    else:\n",
    "        distances = tf.py_func(euclidean_distances,[image_embeddings, text_embeddings], tf.float32)        \n",
    "    return distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "_pairwise_distances(image_embeddings, text_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#No need for positive mask in Multimodal Triplet. Anchor and Positives will be fed through data inputs.\n",
    "\n",
    "def _get_anchor_negative_triplet_mask(labels):\n",
    "    \"\"\"Return a 2D mask where mask[a, n] is True iff a and n have distinct labels.\n",
    "\n",
    "    Args:\n",
    "        labels: tf.int32 `Tensor` with shape [batch_size]\n",
    "\n",
    "    Returns:\n",
    "        mask: tf.bool `Tensor` with shape [batch_size, batch_size]\n",
    "    \"\"\"\n",
    "    # Check if labels[i] != labels[k]\n",
    "    # Uses broadcasting where the 1st argument has shape (1, batch_size) and the 2nd (batch_size, 1)\n",
    "    labels_equal = tf.equal(tf.expand_dims(labels, 0), tf.expand_dims(labels, 1))\n",
    "\n",
    "    mask = tf.logical_not(labels_equal)\n",
    "\n",
    "    return mask\n",
    "\n",
    "\n",
    "def _get_triplet_mask(labels):\n",
    "    \"\"\"Return a 3D mask where mask[a, p, n] is True iff the triplet (a, p, n) is valid.\n",
    "\n",
    "    A triplet (i, j, k) is valid if:\n",
    "        - i == j, k is distinct\n",
    "        - i == j and labels[i] != labels[k]\n",
    "\n",
    "    Args:\n",
    "        labels: tf.int32 `Tensor` with shape [batch_size]\n",
    "    \"\"\"\n",
    "    # Check that i == j and k is distinct\n",
    "    indices_equal = tf.cast(tf.eye(tf.shape(labels)[0]), tf.bool)\n",
    "    indices_not_equal = tf.logical_not(indices_equal)\n",
    "    i_equal_j = tf.expand_dims(indices_equal, 2)\n",
    "    i_not_equal_k = tf.expand_dims(indices_not_equal, 1)\n",
    "    j_not_equal_k = tf.expand_dims(indices_not_equal, 0)\n",
    "\n",
    "    valid_indices = tf.logical_and(tf.logical_and(i_equal_j, i_not_equal_k), j_not_equal_k)\n",
    "\n",
    "\n",
    "    # Check if labels[i] == labels[j] and labels[i] != labels[k]\n",
    "    label_equal = tf.equal(tf.expand_dims(labels, 0), tf.expand_dims(labels, 1))\n",
    "    i_equal_j = tf.expand_dims(label_equal, 2)\n",
    "    i_equal_k = tf.expand_dims(label_equal, 1)\n",
    "\n",
    "    valid_labels = tf.logical_and(i_equal_j, tf.logical_not(i_equal_k))\n",
    "\n",
    "    # Combine the two masks\n",
    "    mask = tf.logical_and(valid_indices, valid_labels)\n",
    "\n",
    "    return mask\n",
    "\n",
    "#No need for triplet mask. Negative Mask, serves as triplet mask for both L_tii and L_itt.\n",
    "\n",
    "def batch_all_triplet_loss(labels, image_embeddings, text_embeddings, margin, squared=False):\n",
    "    \"\"\"Build the triplet loss over a batch of embeddings.\n",
    "\n",
    "    We generate all the valid triplets and average the loss over the positive ones.\n",
    "\n",
    "    Args:\n",
    "        labels: labels of the batch, of size (batch_size,)\n",
    "        embeddings: tensor of shape (batch_size, embed_dim)\n",
    "        margin: margin for triplet loss\n",
    "        squared: Boolean. If true, output is the pairwise squared euclidean distance matrix.\n",
    "                 If false, output is the pairwise euclidean distance matrix.\n",
    "\n",
    "    Returns:\n",
    "        triplet_loss: scalar tensor containing the triplet loss\n",
    "    \"\"\"\n",
    "    \n",
    "    # Get the pairwise distance matrix for L_itt\n",
    "    pairwise_dist = _pairwise_distances(image_embeddings, text_embeddings)\n",
    "    \n",
    "    # Get the pairwise distance matrix for L_tii\n",
    "    pairwise_dist_transpose = tf.transpose(pairwise_dist)\n",
    "\n",
    "    # shape (batch_size, batch_size, 1)\n",
    "    #Anchor Positive Dist is same for L_tii and L_itt\n",
    "    #identity_mask = tf.eye(tf.shape(pairwise_dist))\n",
    "    #anchor_positive_dist = tf.expand_dims(tf.multiply(identity_mask,pairwise_dist), 2)\n",
    "    anchor_positive_dist = tf.expand_dims(pairwise_dist, 2)\n",
    "    print(tf.shape(anchor_positive_dist))\n",
    "    #assert anchor_positive_dist.shape[2] == 1, \"{}\".format(anchor_positive_dist.shape)\n",
    "    \n",
    "    # shape (batch_size, 1, batch_size)\n",
    "    anchor_negative_dist = tf.expand_dims(pairwise_dist, 1) #L_itt\n",
    "    anchor_negative_dist_transpose = tf.expand_dims(pairwise_dist_transpose, 1) #L_tii\n",
    "    #assert anchor_negative_dist.shape[1] == 1, \"{}\".format(anchor_negative_dist.shape)\n",
    "    #assert anchor_negative_dist_transpose.shape[1] == 1, \"{}\".format(anchor_negative_dist_transpose.shape)\n",
    "\n",
    "    # Compute a 3D tensor of size (batch_size, batch_size, batch_size)\n",
    "    # triplet_loss[i, j, k] will contain the triplet loss of anchor=i, positive=j, negative=k\n",
    "    # Uses broadcasting where the 1st argument has shape (batch_size, batch_size, 1)\n",
    "    # and the 2nd (batch_size, 1, batch_size)\n",
    "    L_itt = anchor_positive_dist - anchor_negative_dist + margin\n",
    "    L_tii = anchor_positive_dist - anchor_negative_dist_transpose + margin\n",
    "\n",
    "    # Put to zero the invalid triplets\n",
    "    # (where label(a) != label(p) or label(n) == label(a) or a == p)\n",
    "    mask = _get_triplet_mask(labels)\n",
    "    mask = tf.to_float(mask)\n",
    "    L_itt = tf.multiply(mask, L_itt)\n",
    "    L_tii = tf.multiply(mask, L_tii)\n",
    "\n",
    "    # Remove negative losses (i.e. the easy triplets)\n",
    "    L_itt = tf.maximum(L_itt, 0.0)\n",
    "    L_tii = tf.maximum(L_tii, 0.0)\n",
    "    \n",
    "\n",
    "    # Count number of positive triplets (where triplet_loss > 0)\n",
    "    valid_triplets_tii = tf.to_float(tf.greater(L_tii, 1e-16))\n",
    "    valid_triplets_itt = tf.to_float(tf.greater(L_itt, 1e-16))\n",
    "    \n",
    "    num_positive_triplets_tii = tf.reduce_sum(valid_triplets_tii)\n",
    "    num_positive_triplets_itt = tf.reduce_sum(valid_triplets_itt)\n",
    "    \n",
    "    num_valid_triplets = tf.reduce_sum(mask)\n",
    "    fraction_positive_triplets = (num_positive_triplets_tii + num_positive_triplets_itt) / (2*num_valid_triplets + 1e-16)\n",
    "\n",
    "    # Get final mean triplet loss over the positive valid triplets\n",
    "    triplet_loss = (tf.reduce_sum(L_tii) + tf.reduce_sum(L_itt)) / (num_positive_triplets_tii + num_positive_triplets_itt + 1e-16)\n",
    "    \n",
    "    return triplet_loss, fraction_positive_triplets\n",
    "\n",
    "\n",
    "def batch_hard_triplet_loss(labels, embeddings, margin, squared=False):\n",
    "    \"\"\"Build the triplet loss over a batch of embeddings.\n",
    "\n",
    "    For each anchor, we get the hardest positive and hardest negative to form a triplet.\n",
    "\n",
    "    Args:\n",
    "        labels: labels of the batch, of size (batch_size,)\n",
    "        embeddings: tensor of shape (batch_size, embed_dim)\n",
    "        margin: margin for triplet loss\n",
    "        squared: Boolean. If true, output is the pairwise squared euclidean distance matrix.\n",
    "                 If false, output is the pairwise euclidean distance matrix.\n",
    "\n",
    "    Returns:\n",
    "        triplet_loss: scalar tensor containing the triplet loss\n",
    "    \"\"\"\n",
    "    # Get the pairwise distance matrix\n",
    "    pairwise_dist = _pairwise_distances(embeddings, squared=squared)\n",
    "\n",
    "    # For each anchor, get the hardest positive\n",
    "    # First, we need to get a mask for every valid positive (they should have same label)\n",
    "    mask_anchor_positive = _get_anchor_positive_triplet_mask(labels)\n",
    "    mask_anchor_positive = tf.to_float(mask_anchor_positive)\n",
    "\n",
    "    # We put to 0 any element where (a, p) is not valid (valid if a != p and label(a) == label(p))\n",
    "    anchor_positive_dist = tf.multiply(mask_anchor_positive, pairwise_dist)\n",
    "\n",
    "    # shape (batch_size, 1)\n",
    "    hardest_positive_dist = tf.reduce_max(anchor_positive_dist, axis=1, keepdims=True)\n",
    "    tf.summary.scalar(\"hardest_positive_dist\", tf.reduce_mean(hardest_positive_dist))\n",
    "\n",
    "    # For each anchor, get the hardest negative\n",
    "    # First, we need to get a mask for every valid negative (they should have different labels)\n",
    "    mask_anchor_negative = _get_anchor_negative_triplet_mask(labels)\n",
    "    mask_anchor_negative = tf.to_float(mask_anchor_negative)\n",
    "\n",
    "    # We add the maximum value in each row to the invalid negatives (label(a) == label(n))\n",
    "    max_anchor_negative_dist = tf.reduce_max(pairwise_dist, axis=1, keepdims=True)\n",
    "    anchor_negative_dist = pairwise_dist + max_anchor_negative_dist * (1.0 - mask_anchor_negative)\n",
    "\n",
    "    # shape (batch_size,)\n",
    "    hardest_negative_dist = tf.reduce_min(anchor_negative_dist, axis=1, keepdims=True)\n",
    "    tf.summary.scalar(\"hardest_negative_dist\", tf.reduce_mean(hardest_negative_dist))\n",
    "\n",
    "    # Combine biggest d(a, p) and smallest d(a, n) into final triplet loss\n",
    "    triplet_loss = tf.maximum(hardest_positive_dist - hardest_negative_dist + margin, 0.0)\n",
    "\n",
    "    # Get final mean triplet loss\n",
    "    triplet_loss = tf.reduce_mean(triplet_loss)\n",
    "\n",
    "    return triplet_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ayush/anaconda3/lib/python3.6/site-packages/keras_applications/resnet50.py:265: UserWarning: The output shape of `ResNet50(include_top=False)` has been changed since Keras 2.2.0.\n",
      "  warnings.warn('The output shape of `ResNet50(include_top=False)` '\n"
     ]
    }
   ],
   "source": [
    "\n",
    "image_branch=Sequential()\n",
    "resnet50_model = keras.applications.resnet50.ResNet50(include_top=False, weights=\"imagenet\", input_shape=(512,512,3))\n",
    "image_branch.add(resnet50_model)\n",
    "#model.add(keras.layers.pooling.AveragePooling2D(pool_size=(2, 2), strides=None, border_mode='valid', dim_ordering='default'))\n",
    "#model.add(Flatten())\n",
    "image_branch.add(keras.layers.GlobalAveragePooling2D(data_format='channels_last'))\n",
    "image_branch.add(Dense(1024,kernel_initializer=glorot_normal(seed=None), activation='sigmoid'))\n",
    "image_branch.add(Dense(512, kernel_initializer=glorot_normal(seed=None), activation=None))\n",
    "image_branch.add(keras.layers.BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001, center=True, scale=True, beta_initializer='zeros', gamma_initializer='ones', moving_mean_initializer='zeros', moving_variance_initializer='ones', beta_regularizer=None, gamma_regularizer=None, beta_constraint=None, gamma_constraint=None))\n",
    "\n",
    "text_branch = Sequential()\n",
    "text_branch.add(Dense(400, kernel_initializer=glorot_normal(seed=None), activation='sigmoid', input_shape = (200,)))\n",
    "text_branch.add(Dense(512, kernel_initializer=glorot_normal(seed=None), activation = None))\n",
    "text_branch.add(keras.layers.BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001, center=True, scale=True, beta_initializer='zeros', gamma_initializer='ones', moving_mean_initializer='zeros', moving_variance_initializer='ones', beta_regularizer=None, gamma_regularizer=None, beta_constraint=None, gamma_constraint=None))\n",
    "\n",
    "model_combined=Model(inputs=[image_branch.input, text_branch.input], outputs=[image_branch.output, text_branch.output])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Shape_13:0\", shape=(?,), dtype=int32)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "('Could not interpret loss function identifier:', (<tf.Tensor 'truediv_5:0' shape=() dtype=float32>, <tf.Tensor 'truediv_4:0' shape=() dtype=float32>))",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-71-d6721969a34c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mmodel_loss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0msgd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptimizers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSGD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecay\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-6\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmomentum\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.9\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnesterov\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0mmodel_combined\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msgd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mcompile\u001b[0;34m(self, optimizer, loss, metrics, loss_weights, sample_weight_mode, weighted_metrics, target_tensors, **kwargs)\u001b[0m\n\u001b[1;32m    137\u001b[0m             \u001b[0mloss_functions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mlosses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m             \u001b[0mloss_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlosses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    140\u001b[0m             \u001b[0mloss_functions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mloss_function\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss_functions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_functions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/losses.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(identifier)\u001b[0m\n\u001b[1;32m    138\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m         raise ValueError('Could not interpret '\n\u001b[0;32m--> 140\u001b[0;31m                          'loss function identifier:', identifier)\n\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m: ('Could not interpret loss function identifier:', (<tf.Tensor 'truediv_5:0' shape=() dtype=float32>, <tf.Tensor 'truediv_4:0' shape=() dtype=float32>))"
     ]
    }
   ],
   "source": [
    "labels=[0,1]\n",
    "y_true = labels\n",
    "y_pred = tf.concat([image_embeddings, text_embeddings], 0)\n",
    "\n",
    "#def triplet_loss(image_embeddings, text_embeddings, margin, squared=True):\n",
    "def loss(labels, y_pred):\n",
    "    margin=0.2\n",
    "    image_embeddings, text_embeddings =tf.split(y_pred, num_or_size_splits=2, axis=0)\n",
    "    return batch_all_triplet_loss(labels,image_embeddings, text_embeddings, margin, squared=True)\n",
    "    \n",
    "\n",
    "model_loss=loss(y_true, y_pred)\n",
    "sgd = optimizers.SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "model_combined.compile(loss=model_loss, optimizer=sgd)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
