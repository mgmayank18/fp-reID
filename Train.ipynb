{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Import libs\n",
    "\n",
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import euclidean_distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Read Data Paths\n",
    "\n",
    "images = sorted(glob.glob('/work/cvma/FP/data/*/*[!_xyt]/*'))\n",
    "xyt = sorted(glob.glob('/work/cvma/FP/data/*/*_xyt/*'))\n",
    "image_xyt_pairs = [list(a) for a in zip(images,xyt)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Image Data Preprocessing\n",
    "\n",
    "label_dict = {}\n",
    "label_counter = 0\n",
    "label = []\n",
    "\n",
    "def assign_label(key):\n",
    "    global label_counter\n",
    "    if key in label_dict.keys():\n",
    "        label.append(label_dict[key])\n",
    "        return 0\n",
    "    else:\n",
    "        label_dict[key] = label_counter\n",
    "        label.append(label_counter)\n",
    "        label_counter=label_counter+1\n",
    "        #Do Something\n",
    "        return 0\n",
    "for image in images:\n",
    "    filename = image.split(\"/\")[-1]\n",
    "    foldername = \"/\".join(image.split(\"/\")[0:-1])\n",
    "    tags = filename.split(\"_\")\n",
    "    if len(tags) == 5:\n",
    "        key = foldername+\"/\"+\"_\".join(tags[0:4])\n",
    "        assign_label(key)\n",
    "\n",
    "    elif len(tags) == 3:\n",
    "        key = foldername+\"/\"+tags[0]+\"_\"+tags[2]\n",
    "        assign_label(key)\n",
    "    #elif len(tags) == 4:\n",
    "        \n",
    "    #elif len(tags) == 2:\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Minutae Data Preprocessing\n",
    "def crop_top(df,crop_size):\n",
    "    #this will take top n inputs for df\n",
    "    if df.shape[0]<crop_size:\n",
    "        d_zero = np.zeros((crop_size,df.shape[1]), dtype=int)\n",
    "        d_zero[:df.shape[0], :df.shape[1]]=df\n",
    "        return d_zero\n",
    "    else:\n",
    "        return df[0:crop_size,:]\n",
    "    \n",
    "def preprocess_xyt_file(filename):\n",
    "    df=pd.read_csv(filename, sep=' ',header=None)\n",
    "    df = np.array(df)\n",
    "    np.sort(df, axis=0)\n",
    "    df = df[df[:,3].argsort()[::-1]]\n",
    "\n",
    "    normalization_factor = np.array([512,512,360,100])\n",
    "    df_new = crop_top(df,50)/normalization_factor\n",
    "    \n",
    "    return df_new.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Define functions to create the triplet loss with online triplet mining.\"\"\"\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "image_embeddings = tf.random_uniform([1000,512], minval=0, maxval=1, dtype=tf.float32)\n",
    "image_embeddings = tf.constant([[1.0,1.0,1.0],[3.0,3.0,3.0]])\n",
    "text_embeddings = tf.random_uniform([1000,512], minval=0, maxval=1, dtype=tf.float32)\n",
    "text_embeddings = tf.constant([[1.0,2.0,1.0],[1.0,2.0,3.0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def euclid(image,text):\n",
    "    return euclidean_distances(image,text,squared=True)\n",
    "\n",
    "def _pairwise_distances(image_embeddings, text_embeddings, squared=False):\n",
    "    #Returns Pairwise Euclidean Distances\n",
    "    distances = tf.py_func(euclid, [image_embeddings, text_embeddings], tf.float32)\n",
    "\n",
    "    return distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'PyFunc_6:0' shape=<unknown> dtype=float32>"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_pairwise_distances(image_embeddings, text_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#No need for positive mask in Multimodal Triplet. Anchor and Positives will be fed through data inputs.\n",
    "\n",
    "def _get_anchor_negative_triplet_mask(labels):\n",
    "    \"\"\"Return a 2D mask where mask[a, n] is True iff a and n have distinct labels.\n",
    "\n",
    "    Args:\n",
    "        labels: tf.int32 `Tensor` with shape [batch_size]\n",
    "\n",
    "    Returns:\n",
    "        mask: tf.bool `Tensor` with shape [batch_size, batch_size]\n",
    "    \"\"\"\n",
    "    # Check if labels[i] != labels[k]\n",
    "    # Uses broadcasting where the 1st argument has shape (1, batch_size) and the 2nd (batch_size, 1)\n",
    "    labels_equal = tf.equal(tf.expand_dims(labels, 0), tf.expand_dims(labels, 1))\n",
    "\n",
    "    mask = tf.logical_not(labels_equal)\n",
    "\n",
    "    return mask\n",
    "\n",
    "\n",
    "def _get_triplet_mask(labels):\n",
    "    \"\"\"Return a 3D mask where mask[a, p, n] is True iff the triplet (a, p, n) is valid.\n",
    "\n",
    "    A triplet (i, j, k) is valid if:\n",
    "        - i == j, k is distinct\n",
    "        - i == j and labels[i] != labels[k]\n",
    "\n",
    "    Args:\n",
    "        labels: tf.int32 `Tensor` with shape [batch_size]\n",
    "    \"\"\"\n",
    "    # Check that i == j and k is distinct\n",
    "    indices_equal = tf.cast(tf.eye(tf.shape(labels)[0]), tf.bool)\n",
    "    indices_not_equal = tf.logical_not(indices_equal)\n",
    "    i_equal_j = tf.expand_dims(indices_equal, 2)\n",
    "    i_not_equal_k = tf.expand_dims(indices_not_equal, 1)\n",
    "    j_not_equal_k = tf.expand_dims(indices_not_equal, 0)\n",
    "\n",
    "    valid_indices = tf.logical_and(tf.logical_and(i_equal_j, i_not_equal_k), j_not_equal_k)\n",
    "\n",
    "\n",
    "    # Check if labels[i] == labels[j] and labels[i] != labels[k]\n",
    "    label_equal = tf.equal(tf.expand_dims(labels, 0), tf.expand_dims(labels, 1))\n",
    "    i_equal_j = tf.expand_dims(label_equal, 2)\n",
    "    i_equal_k = tf.expand_dims(label_equal, 1)\n",
    "\n",
    "    valid_labels = tf.logical_and(i_equal_j, tf.logical_not(i_equal_k))\n",
    "\n",
    "    # Combine the two masks\n",
    "    mask = tf.logical_and(valid_indices, valid_labels)\n",
    "\n",
    "    return mask\n",
    "\n",
    "#No need for triplet mask. Negative Mask, serves as triplet mask for both L_tii and L_itt.\n",
    "\n",
    "def batch_all_triplet_loss(labels, image_embeddings, text_embeddings, margin, squared=False):\n",
    "    \"\"\"Build the triplet loss over a batch of embeddings.\n",
    "\n",
    "    We generate all the valid triplets and average the loss over the positive ones.\n",
    "\n",
    "    Args:\n",
    "        labels: labels of the batch, of size (batch_size,)\n",
    "        embeddings: tensor of shape (batch_size, embed_dim)\n",
    "        margin: margin for triplet loss\n",
    "        squared: Boolean. If true, output is the pairwise squared euclidean distance matrix.\n",
    "                 If false, output is the pairwise euclidean distance matrix.\n",
    "\n",
    "    Returns:\n",
    "        triplet_loss: scalar tensor containing the triplet loss\n",
    "    \"\"\"\n",
    "    # Get the pairwise distance matrix for L_itt\n",
    "    pairwise_dist = _pairwise_distances(image_embeddings, text_embeddings)\n",
    "\n",
    "    # shape (batch_size, batch_size, 1)\n",
    "    identity_mask = tf.eye(tf.shape(pairwise_dist))\n",
    "    anchor_positive_dist = tf.expand_dims(tf.multiply(identity_mask,pairwise_dist), 2)\n",
    "    assert anchor_positive_dist.shape[2] == 1, \"{}\".format(anchor_positive_dist.shape)\n",
    "    # shape (batch_size, 1, batch_size)\n",
    "    anchor_negative_dist = tf.expand_dims(pairwise_dist, 1)\n",
    "    assert anchor_negative_dist.shape[1] == 1, \"{}\".format(anchor_negative_dist.shape)\n",
    "\n",
    "    # Compute a 3D tensor of size (batch_size, batch_size, batch_size)\n",
    "    # triplet_loss[i, j, k] will contain the triplet loss of anchor=i, positive=j, negative=k\n",
    "    # Uses broadcasting where the 1st argument has shape (batch_size, batch_size, 1)\n",
    "    # and the 2nd (batch_size, 1, batch_size)\n",
    "    triplet_loss = anchor_positive_dist - anchor_negative_dist + margin\n",
    "\n",
    "    # Put to zero the invalid triplets\n",
    "    # (where label(a) != label(p) or label(n) == label(a) or a == p)\n",
    "    mask = _get_triplet_mask(labels)\n",
    "    mask = tf.to_float(mask)\n",
    "    triplet_loss = tf.multiply(mask, triplet_loss)\n",
    "\n",
    "    # Remove negative losses (i.e. the easy triplets)\n",
    "    triplet_loss = tf.maximum(triplet_loss, 0.0)\n",
    "\n",
    "    # Count number of positive triplets (where triplet_loss > 0)\n",
    "    valid_triplets = tf.to_float(tf.greater(triplet_loss, 1e-16))\n",
    "    num_positive_triplets = tf.reduce_sum(valid_triplets)\n",
    "    num_valid_triplets = tf.reduce_sum(mask)\n",
    "    fraction_positive_triplets = num_positive_triplets / (num_valid_triplets + 1e-16)\n",
    "\n",
    "    # Get final mean triplet loss over the positive valid triplets\n",
    "    triplet_loss = tf.reduce_sum(triplet_loss) / (num_positive_triplets + 1e-16)\n",
    "    \n",
    "    # L_tii\n",
    "    pairwise_dist_transpose = tf.transpose(pairwise_dist)\n",
    "    \n",
    "    return triplet_loss, fraction_positive_triplets\n",
    "\n",
    "\n",
    "def batch_hard_triplet_loss(labels, embeddings, margin, squared=False):\n",
    "    \"\"\"Build the triplet loss over a batch of embeddings.\n",
    "\n",
    "    For each anchor, we get the hardest positive and hardest negative to form a triplet.\n",
    "\n",
    "    Args:\n",
    "        labels: labels of the batch, of size (batch_size,)\n",
    "        embeddings: tensor of shape (batch_size, embed_dim)\n",
    "        margin: margin for triplet loss\n",
    "        squared: Boolean. If true, output is the pairwise squared euclidean distance matrix.\n",
    "                 If false, output is the pairwise euclidean distance matrix.\n",
    "\n",
    "    Returns:\n",
    "        triplet_loss: scalar tensor containing the triplet loss\n",
    "    \"\"\"\n",
    "    # Get the pairwise distance matrix\n",
    "    pairwise_dist = _pairwise_distances(embeddings, squared=squared)\n",
    "\n",
    "    # For each anchor, get the hardest positive\n",
    "    # First, we need to get a mask for every valid positive (they should have same label)\n",
    "    mask_anchor_positive = _get_anchor_positive_triplet_mask(labels)\n",
    "    mask_anchor_positive = tf.to_float(mask_anchor_positive)\n",
    "\n",
    "    # We put to 0 any element where (a, p) is not valid (valid if a != p and label(a) == label(p))\n",
    "    anchor_positive_dist = tf.multiply(mask_anchor_positive, pairwise_dist)\n",
    "\n",
    "    # shape (batch_size, 1)\n",
    "    hardest_positive_dist = tf.reduce_max(anchor_positive_dist, axis=1, keepdims=True)\n",
    "    tf.summary.scalar(\"hardest_positive_dist\", tf.reduce_mean(hardest_positive_dist))\n",
    "\n",
    "    # For each anchor, get the hardest negative\n",
    "    # First, we need to get a mask for every valid negative (they should have different labels)\n",
    "    mask_anchor_negative = _get_anchor_negative_triplet_mask(labels)\n",
    "    mask_anchor_negative = tf.to_float(mask_anchor_negative)\n",
    "\n",
    "    # We add the maximum value in each row to the invalid negatives (label(a) == label(n))\n",
    "    max_anchor_negative_dist = tf.reduce_max(pairwise_dist, axis=1, keepdims=True)\n",
    "    anchor_negative_dist = pairwise_dist + max_anchor_negative_dist * (1.0 - mask_anchor_negative)\n",
    "\n",
    "    # shape (batch_size,)\n",
    "    hardest_negative_dist = tf.reduce_min(anchor_negative_dist, axis=1, keepdims=True)\n",
    "    tf.summary.scalar(\"hardest_negative_dist\", tf.reduce_mean(hardest_negative_dist))\n",
    "\n",
    "    # Combine biggest d(a, p) and smallest d(a, n) into final triplet loss\n",
    "    triplet_loss = tf.maximum(hardest_positive_dist - hardest_negative_dist + margin, 0.0)\n",
    "\n",
    "    # Get final mean triplet loss\n",
    "    triplet_loss = tf.reduce_mean(triplet_loss)\n",
    "\n",
    "    return triplet_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "labels = [0,1,1]\n",
    "indices_equal = tf.cast(tf.eye(tf.shape(labels)[0]), tf.bool)\n",
    "indices_not_equal = tf.logical_not(indices_equal)\n",
    "Triplet_Mask=_get_triplet_mask(labels)\n",
    "Triplet_Mask[1,1,2].eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
